import os
import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance
from joblib import dump


def main(input_path, output_dir):
    # ========== 1. 加载数据 ==========
    print(f" 正在读取文件：{input_path}")
    if input_path.endswith(".xlsx") or input_path.endswith(".xls"):
        data = pd.read_excel(input_path)
    elif input_path.endswith(".csv"):
        data = pd.read_csv(input_path)
    else:
        raise ValueError(" 仅支持 Excel 或 CSV 文件！")

    print(f" 数据读取成功：{data.shape[0]} 行，{data.shape[1]} 列")

    # 自动选择前 9 列为特征，其余列为目标变量
    if data.shape[1] <= 9:
        raise ValueError(" 列数不足！请确保前 9 列为特征，后面为目标变量。")

    X = data.iloc[:, 0:9]
    y_columns = data.columns[9:]

    # 创建输出目录
    model_dir = os.path.join(output_dir, "models")
    os.makedirs(model_dir, exist_ok=True)

    # ========== 2. 定义模型与参数 ==========
    models = {
        'XGBoost': XGBRegressor(),
        'Random Forest': RandomForestRegressor(),
        'K Nearest Neighbors': KNeighborsRegressor()
    }

    param_grids = {
        'XGBoost': {
            'xgboost__n_estimators': [100, 300, 500],
            'xgboost__max_depth': [3, 5, 7],
            'xgboost__learning_rate': [0.05, 0.1, 0.2]
        },
        'Random Forest': {
            'randomforest__n_estimators': [100, 300, 500],
            'randomforest__max_depth': [None, 10, 20],
            'randomforest__min_samples_split': [2, 5]
        },
        'K Nearest Neighbors': {
            'knearestneighbors__n_neighbors': [3, 5, 7, 10],
            'knearestneighbors__weights': ['uniform', 'distance'],
            'knearestneighbors__metric': ['euclidean', 'manhattan']
        }
    }

    # ========== 3. 初始化结果容器 ==========
    results = []
    feature_importances = []
    all_cv_results = []

    # ========== 4. 模型训练循环 ==========
    for y_column in y_columns:
        print(f"\n Processing target variable: {y_column}")
        y = data[y_column]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=20250521)

        for model_name, model in models.items():
            print(f"    Training model: {model_name}")
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                (model_name.lower().replace(' ', ''), model)
            ])

            grid_search = GridSearchCV(
                pipeline,
                param_grids[model_name],
                cv=10,
                scoring='r2',
                refit=True,
                n_jobs=-1
            )

            grid_search.fit(X_train, y_train)

            # 保存交叉验证结果
            cv_results = pd.DataFrame(grid_search.cv_results_)
            cv_results['Model'] = model_name
            cv_results['Target'] = y_column
            all_cv_results.append(cv_results)

            best_params = grid_search.best_params_
            y_pred_test = grid_search.predict(X_test)
            y_pred_all = grid_search.predict(X)

            # 测试集与全样本性能
            r2_test = r2_score(y_test, y_pred_test)
            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
            r2_all = r2_score(y, y_pred_all)
            rmse_all = np.sqrt(mean_squared_error(y, y_pred_all))

            # 保存最优模型
            model_path = os.path.join(model_dir, f"{y_column}_{model_name.replace(' ', '_')}_best.pkl")
            dump(grid_search.best_estimator_, model_path)

            # ---------- 特征重要性 ----------
            if model_name == 'XGBoost':
                model_inner = grid_search.best_estimator_.named_steps['xgboost']
                for f, imp in zip(X.columns, model_inner.feature_importances_):
                    feature_importances.append({'Target': y_column, 'Model': model_name, 'Feature': f, 'Importance': imp})

            elif model_name == 'Random Forest':
                model_inner = grid_search.best_estimator_.named_steps['randomforest']
                for f, imp in zip(X.columns, model_inner.feature_importances_):
                    feature_importances.append({'Target': y_column, 'Model': model_name, 'Feature': f, 'Importance': imp})

            elif model_name == 'K Nearest Neighbors':
                model_inner = grid_search.best_estimator_
                result = permutation_importance(model_inner, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
                for f, imp in zip(X.columns, result.importances_mean):
                    feature_importances.append({'Target': y_column, 'Model': model_name, 'Feature': f, 'Importance': imp})

            # 记录结果
            results.append({
                'Target': y_column,
                'Model': model_name,
                'Best Parameters': str(best_params),
                'R2_test': r2_test,
                'RMSE_test': rmse_test,
                'R2_all': r2_all,
                'RMSE_all': rmse_all,
                'Model Path': model_path
            })

    # ========== 5. 保存所有结果 ==========
    print("\n 正在保存结果...")
    results_df = pd.DataFrame(results)
    features_df = pd.DataFrame(feature_importances)
    cv_df = pd.concat(all_cv_results, ignore_index=True)

    os.makedirs(output_dir, exist_ok=True)
    results_df.to_excel(os.path.join(output_dir, "模型评估结果.xlsx"), index=False)
    features_df.to_excel(os.path.join(output_dir, "特征重要性结果.xlsx"), index=False)
    cv_df.to_excel(os.path.join(output_dir, "交叉验证详细结果.xlsx"), index=False)

    print(f" 所有文件已保存至：{output_dir}")
    print(" 任务完成！")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="多模型回归与特征重要性分析")
    parser.add_argument("--input", required=True, help="输入 Excel 或 CSV 文件路径")
    parser.add_argument("--output_dir", required=True, help="输出文件夹路径")
    args = parser.parse_args()

    main(args.input, args.output_dir)
